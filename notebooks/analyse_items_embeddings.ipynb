{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from srgnn_model import SRGNN_model\n",
    "from srgnn_datasets import SRGNN_Map_Dataset, SRGNN_sampler\n",
    "from utils import fake_parser\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from math import ceil\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_id='run-20240404_162708-ekuo66ei' # not optimal hparams diginetica\n",
    "#run_id='run-20240531_122335-i78k1rzu' # diginetica\n",
    "run_id='jxgwsuta'\n",
    "#run_id='run-20240422_103727-ex2zwqx6' # yoochoose\n",
    "\n",
    "\n",
    "#finetuned_run_id='run-20240302_233004-xh5dmcet'\n",
    "global_run_id=run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt=load_model(run_id, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pickle.load(open('../datasets/' + opt.dataset + '/test.txt', 'rb'))\n",
    "test_dataset=SRGNN_Map_Dataset(test_data, shuffle=False)\n",
    "test_dataloader=DataLoader(test_dataset, \n",
    "                            num_workers=os.cpu_count(),  \n",
    "                            sampler=SRGNN_sampler(test_dataset, opt.batchSize, shuffle=False, drop_last=False),\n",
    "                             drop_last=False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df=pd.read_csv(f'../datasets/{opt.dataset}/items.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sessions, test_targets, test_sids=test_data[:3]\n",
    "test_session_ids=set(map(int, test_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit,mrr=[],[]\n",
    "\n",
    "model.to('cuda')\n",
    "for batch in tqdm(test_dataloader, total=test_dataset.length//opt.batchSize):\n",
    "    batch=[b.to('cuda') for b in batch]\n",
    "\n",
    "    sub_scores, targets=model.predict_step(batch)\n",
    "    targets=targets.flatten()\n",
    "    for score, target in zip(sub_scores, targets):\n",
    "        correct_pred=torch.isin(target - 1, score).cpu()\n",
    "        hit.append(correct_pred)\n",
    "        if not correct_pred:\n",
    "            mrr.append(0)\n",
    "        else:\n",
    "            mrr.append(1 / (torch.where(score == target - 1)[0][0] + 1).cpu())\n",
    "\n",
    "model.to('cpu')\n",
    "hit=np.array(hit)\n",
    "mrr=np.array(mrr)\n",
    "print('hit ', np.average(hit), 'mrr ', np.average(mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_len=[]\n",
    "session_frequency=[]\n",
    "session_categories=[]\n",
    "target_category=[]\n",
    "for idx in tqdm(range(len(test_sessions))):\n",
    "    sess_items_df=items_df.loc[items_df.item_number.isin(test_sessions[idx])]\n",
    "    session_len.append(len(test_sessions[idx]))\n",
    "    session_frequency.append(np.average(sess_items_df.frequency))\n",
    "    session_categories.append(sess_items_df.category.nunique())\n",
    "\n",
    "    sess_target_categories=items_df.loc[items_df.item_number==test_targets[idx]].category\n",
    "    target_category.append(any([c in sess_items_df.category.values for c in sess_target_categories]))\n",
    "\n",
    "session_len=np.array(session_len)\n",
    "session_frequency=np.array(session_frequency)\n",
    "session_categories=np.array(session_categories)\n",
    "target_category=np.array(target_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df=pd.DataFrame(np.vstack([session_len, session_frequency, session_categories, target_category, test_targets, test_sids, hit, mrr]).T,\n",
    "                        columns=['length','frequency','no_categories','target_category','target','session_id', 'hit','mrr'],\n",
    "                        ).sort_values('session_id').reset_index(drop=True)\n",
    "session_df.session_id=session_df.session_id.astype(int)\n",
    "session_df.target=session_df.target.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# items embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_embedding(model, item_ids: torch.tensor):\n",
    "    return model.model.embedding(item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_embeddings=get_items_embedding(model, torch.arange(items_df.item_number.nunique()+1, device=model.device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_clusters=16\n",
    "#gm=GaussianMixture(n_components=no_clusters, n_init=2, init_params='k-means++', weights_init=np.ones(no_clusters)/no_clusters)\n",
    "#item_labels=gm.fit_predict(items_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters=8\n",
    "init_params='k-means++'\n",
    "covariance_type='full'\n",
    "tol=1e-3\n",
    "\n",
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/item_labels_gmm_{no_clusters}_{init_params}_{covariance_type}_{tol}_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    item_labels=pickle.load(f)\n",
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/cluster_centers_gmm_{no_clusters}_{init_params}_{covariance_type}_{tol}_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    gm_means=pickle.load( f)\n",
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/gmm_model_{no_clusters}_{init_params}_{covariance_type}_{tol}_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    gm_model=pickle.load( f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/item_labels_kmeans_{no_clusters}_{init_params}_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    item_labels_kmeans=pickle.load(f)\n",
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/cluster_centers_kmeans_{no_clusters}_{init_params}_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    kmeans_means=pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    f\"../datasets/{opt.dataset}/cluster_centers_categories_{opt.hiddenSize}_{run_id.split('-')[-1]}.txt\",\n",
    "    \"rb\",\n",
    ") as f:\n",
    "    categories_means=pickle.load( f)\n",
    "    categories_means=np.asarray(list(categories_means.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_cluster_distance=np.zeros((no_clusters, no_clusters))\n",
    "kmeans_cluster_distance=np.zeros((no_clusters, no_clusters))\n",
    "categories_cluster_distance=np.zeros((len(categories_means), len(categories_means)))\n",
    "\n",
    "for i in range(no_clusters):\n",
    "    gm_cluster_distance[i] = np.linalg.norm(\n",
    "            gm_means - gm_means[i], axis=1\n",
    "        )\n",
    "    kmeans_cluster_distance[i] = np.linalg.norm(\n",
    "            kmeans_means - kmeans_means[i], axis=1\n",
    "        )\n",
    "for i in range(len(categories_cluster_distance)):\n",
    "    categories_cluster_distance[i] = np.linalg.norm(\n",
    "            categories_means - categories_means[i], axis=1\n",
    "        )\n",
    "\n",
    "#gm_cluster_distance = 1 / gm_cluster_distance ########   1/x yields WEIGHTS FOR ADJACENCY MATRIX\n",
    "#kmeans_cluster_distance = 1 / kmeans_cluster_distance\n",
    "gm_cluster_distance[gm_cluster_distance==0]=np.nan\n",
    "kmeans_cluster_distance[kmeans_cluster_distance==0]=np.nan\n",
    "#categories_cluster_distance = 1 / categories_cluster_distance\n",
    "categories_cluster_distance[categories_cluster_distance==0]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nanmin(gm_cluster_distance),  np.nanmax(gm_cluster_distance),) \n",
    "print(np.nanmin(kmeans_cluster_distance), np.nanmax(kmeans_cluster_distance))\n",
    "print(np.nanmin(categories_cluster_distance), np.nanmax(categories_cluster_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for c in range(no_clusters):\n",
    "    fig.add_trace(go.Scatter(x=[tsne_items_embeddings[-2*no_clusters+c, 0]], y=[tsne_items_embeddings[-2*no_clusters+c, 1]], \n",
    "                            name='gmm'+str(c), mode='markers', \n",
    "                            legendgroup='gmm',\n",
    "                            marker=dict(size=12,\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "for c in range(no_clusters):\n",
    "    fig.add_trace(go.Scatter(x=[tsne_items_embeddings[-no_clusters+c, 0]], y=[tsne_items_embeddings[-no_clusters+c, 1]], \n",
    "                            name='kmeans'+str(c), mode='markers', \n",
    "                            legendgroup='kmeans',\n",
    "                            marker=dict(size=12,\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "\n",
    "fig.update_layout(title=f'TSNE reduced items embeddings with GM with {init_params} init, tol {tol},{covariance_type} covariance matrix',\n",
    "                margin=dict(l=40, r=40, t=40, b=40),\n",
    "                width=1000, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(gm_cluster_distance):\n",
    "    print(str(j)+' '+str(np.round(i, decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(kmeans_cluster_distance):\n",
    "    print(str(j)+' '+str(np.round(i, decimals=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=gm_model.predict_proba(items_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b=np.max(a, axis=1)\n",
    "plt.hist(b, bins=np.linspace(np.percentile(b, 15), np.percentile(b, 100), num=30), cumulative=False, density=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne=TSNE(2, init='random', early_exaggeration=32, verbose=1)\n",
    "tsne_items_embeddings=tsne.fit_transform(np.vstack([items_embeddings, gm_means, kmeans_means]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,a = np.unique(item_labels, return_counts=True)\n",
    "_,b = np.unique(item_labels_kmeans, return_counts=True)\n",
    "for i in range(no_clusters):\n",
    "    print(i, f'gmm:  {a[i]:<6}, kmeans: {b[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm as progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for perp, ea, init_alg in progress_bar(product([8, 16, 32, 64], [8,16,32,64], ['random','pca']), total=32):\n",
    "\n",
    "#tsne=TSNE(2, init=init_alg, early_exaggeration=ea, perplexity=perp, verbose=0, n_iter=2000)\n",
    "#tsne_items_embeddings=tsne.fit_transform(np.vstack([items_embeddings, gm_means, kmeans_means]))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in np.unique(item_labels):\n",
    "    label_embedding=tsne_items_embeddings[:-2*no_clusters][item_labels==label]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), mode='markers'))\n",
    "\n",
    "for c in range(no_clusters):\n",
    "    fig.add_trace(go.Scatter(x=[tsne_items_embeddings[-2*no_clusters+c, 0]], y=[tsne_items_embeddings[-2*no_clusters+c, 1]], \n",
    "                            name=str(c), mode='markers', \n",
    "                            marker=dict(size=12,\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f'TSNE reduced items embeddings with GM with {init_params} init, tol {tol},{covariance_type} covariance matrix',\n",
    "                margin=dict(l=40, r=40, t=40, b=40),\n",
    "                width=1000, height=800)\n",
    "fig.write_html(f'./images/items_tsne_{tsne.init}_perpexlity_{tsne.perplexity}_ea_{tsne.early_exaggeration}_GMM_{no_clusters}_{init_params}_{covariance_type}_{tol}_{opt.dataset}_{opt.hiddenSize}_{global_run_id.split(\"-\")[-1]}.html')\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for label in np.unique(item_labels_kmeans):\n",
    "    label_embedding=tsne_items_embeddings[:-2*no_clusters][item_labels_kmeans==label]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), mode='markers'))\n",
    "\n",
    "for c in range(no_clusters):\n",
    "    fig.add_trace(go.Scatter(x=[tsne_items_embeddings[-no_clusters+c, 0]], y=[tsne_items_embeddings[-no_clusters+c, 1]], \n",
    "                             name=str(c), mode='markers', \n",
    "                             marker=dict(size=12,\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "\n",
    "\n",
    "fig.update_layout(title='TSNE reduced items embeddings with Kmeans',\n",
    "                  margin=dict(l=40, r=40, t=40, b=40),\n",
    "                  width=1000, height=800)\n",
    "fig.write_html(f'./images/items_tsne_KMEANS_{tsne.init}_{no_clusters}_{init_params}_{opt.dataset}_{opt.hiddenSize}_{global_run_id.split(\"-\")[-1]}.html')\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single session example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/' + opt.dataset + '/test.txt', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDXS=[len(i)>8 for i in test_data[0]]\n",
    "long_sessions=np.arange(len(test_data[0]))[IDXS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=np.random.choice(long_sessions)\n",
    "\n",
    "seqence=test_data[0][idx]\n",
    "target=test_data[1][idx]\n",
    "idx, mrr[idx], item_labels[seqence], item_labels[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(1, len(seqence)):\n",
    "    print(f'{i-1}->{i}',np.linalg.norm(items_embeddings[seqence[i]]-items_embeddings[seqence[i-1]]))\n",
    "print('last->target',np.linalg.norm(items_embeddings[seqence[-1]]-items_embeddings[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "colors=px.colors.qualitative.Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for label in np.unique(item_labels):#[np.unique(item_labels, return_counts=True)[1]>len(item_labels)/ngmm]:\n",
    "    label_embedding=tsne_items_embeddings[:-2*no_clusters][item_labels==label]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), mode='markers', opacity=1))\n",
    "\n",
    "for i, item in enumerate(seqence):#[np.unique(item_labels, return_counts=True)[1]>len(item_labels)/ngmm]:\n",
    "    sequence_embedding=tsne_items_embeddings[item]\n",
    "    fig.add_trace(go.Scatter(x=[sequence_embedding[0]], y=[sequence_embedding[1]], \n",
    "                             name=f'item_{i}', mode='markers', \n",
    "                             marker=dict(size=20,\n",
    "                                         color=colors[item_labels[item]],\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "    \n",
    "fig.add_trace(go.Scatter(x=[tsne_items_embeddings[target][0]], y=[tsne_items_embeddings[target][1]], \n",
    "                             name=f'target', mode='markers', \n",
    "                            marker=dict(size=30,\n",
    "                                      color=colors[item_labels[target]],\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "\n",
    "sequence_embedding=tsne_items_embeddings[seqence]\n",
    "fig.add_trace(go.Scatter(x=sequence_embedding[:, 0], y=sequence_embedding[:, 1], \n",
    "                             name='session', mode='markers+lines', \n",
    "                             marker=dict(symbol=\"arrow\",\n",
    "                                        size=15,\n",
    "                                        angleref=\"previous\",\n",
    "                                        color='Black')\n",
    "                                        ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[sequence_embedding[-1, 0], tsne_items_embeddings[target][0]],\n",
    "                          y=[sequence_embedding[-1, 1], tsne_items_embeddings[target][1]], \n",
    "                             name='prediciton', mode='markers+lines', \n",
    "                             marker=dict(symbol=\"arrow\",\n",
    "                                        size=15,\n",
    "                                        angleref=\"previous\",\n",
    "                                        color='Red')\n",
    "                                        ))\n",
    "    \n",
    "fig.update_layout(title='',\n",
    "                  margin=dict(l=40, r=40, t=40, b=40),\n",
    "                  width=1000, height=800)\n",
    "fig.write_html(f'./images/sequence_tsne_{tsne.init}_{opt.dataset}_{no_clusters}_{run_id.split(\"-\")[-1]}.html')\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.loc[items_df.item_number.isin(seqence)].copy().set_index('item_number').loc[seqence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERP info (diginetica only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df=df=pd.read_csv('../datasets/train-item-views.csv', sep=';', \n",
    "                          names=['session_id','userId','item_id','timeframe','eventdate'], \n",
    "                          dtype={'item_id':str}).sort_values(by=['session_id','timeframe']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu_df=pd.read_csv('../datasets/train-queries.csv', sep=';').rename(columns={\n",
    "    'sessionId':'session_id','categoryId':'category','items':'serp', 'queryId':'query_id'\n",
    "}).sort_values(by=['session_id','timeframe','query_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_for_session_clicks(sid):\n",
    "    q=qu_df.loc[qu_df.session_id==sid]\n",
    "    s=clicks_df.loc[clicks_df.session_id==sid]\n",
    "    qids=[]\n",
    "    serps=[]\n",
    "    for timeframe in s.timeframe:\n",
    "        idxs=(q.timeframe<timeframe).values\n",
    "        vals=q.query_id.values[idxs]\n",
    "        s_vals=q.serp.values[idxs]\n",
    "        qid=np.nan\n",
    "        serp=np.nan\n",
    "        if len(vals):\n",
    "            qid=vals[-1]\n",
    "            serp=s_vals[-1]\n",
    "        qids.append(qid)\n",
    "        serps.append(serp)\n",
    "    return (sid, qids, serps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "serp_df=pd.DataFrame([get_query_for_session_clicks(sid) for sid in clicks_df.session_id.unique()], columns=['session_id','query_id','serp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['query_id']=serp_df.explode(column=['query_id','serp']).query_id.values\n",
    "clicks_df['serp']=serp_df.explode(column=['query_id','serp']).serp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_pos_in_serp(row):\n",
    "    if row.query_id is np.nan:\n",
    "        return -1\n",
    "    serp=row.serp.split(',')\n",
    "    if str(row.item_id) in serp:\n",
    "        return serp.index(str(row.item_id))/len(row.serp)\n",
    "    else: return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['serp_pos']=clicks_df.apply(lambda r: item_pos_in_serp(r), axis=1, )\n",
    "clicks_df['serp_len']=clicks_df.apply(lambda r: np.nan if r.serp is np.nan else len(r.serp.split(',')), axis=1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicks_df.to_csv('../datasets/diginetica/clicks_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicks_df=pd.read_csv('../datasets/diginetica/clicks_df.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of catagories in both queries and items: ', len(set(qu_df.category.unique()) & set(items_df.category.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict={}\n",
    "for _, r in tqdm(items_df.iterrows()):\n",
    "    item_dict[r.item_id]=r.item_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serp_center(qid, serp):\n",
    "    if serp is np.nan:\n",
    "        return (qid, np.nan)\n",
    "    items=[]\n",
    "    for i in serp.split(','):\n",
    "        if int(i) in item_dict.keys():\n",
    "            items.append(item_dict[int(i)])\n",
    "    if items:\n",
    "        embs=get_items_embedding(model, torch.tensor(items, device=model.device)).cpu().detach().numpy()\n",
    "        return (qid, np.average(embs, axis=0))\n",
    "    return (qid, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_serp_dict=(clicks_df[['query_id', 'serp']].drop_duplicates().apply(lambda r: serp_center(r.query_id, r.serp), axis=1))\n",
    "q_serp_dict=dict([x for x in q_serp_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_form_serp(r):\n",
    "    if (int(r.item_id) in item_dict.keys()) and not np.isnan(r.query_id):\n",
    "        return np.linalg.norm(items_embeddings[item_dict[int(r.item_id)]]-q_serp_dict[r.query_id]) \n",
    "    else: return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_from_serp=clicks_df.apply(lambda r: get_dist_form_serp(r), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['dist_from_serp']=dist_from_serp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_from_serp, bins=100)\n",
    "plt.title('Distribution: Distance of item emb from SERP item embeddings center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df.loc[clicks_df.session_id.isin(test_session_ids)].dist_from_serp, bins=100)\n",
    "plt.title('Same but for test sessions only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['item_number']=clicks_df.item_id.map(lambda x: item_dict[int(x)] if int(x) in item_dict.keys() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df.loc[session_df.session_id==100083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.loc[clicks_df.session_id==100083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.serp_pos=clicks_df.serp_pos.map(lambda x: x if x>=0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_search_query(qid):\n",
    "    st=qu_df.loc[qu_df.query_id==qid]['searchstring.tokens']\n",
    "    if st.shape[0]!=1:\n",
    "        return False\n",
    "    st=st.item()\n",
    "    if isinstance(st, float):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_dict={}\n",
    "for qid in (clicks_df.query_id.unique()):#, total=clicks_df.query_id.nunique()):\n",
    "    search_query_dict[qid]=is_search_query(qid)\n",
    "\n",
    "sq_idxs=clicks_df.query_id.map(lambda x: x if np.isnan(x) else search_query_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df['search_query']=sq_idxs\n",
    "clicks_df['serp_abs_pos']=(clicks_df.serp_pos*clicks_df.serp_len).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.to_csv('../datasets/diginetica/clicks_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reload clicks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df=pd.read_csv('../datasets/diginetica/clicks_df.csv').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=clicks_df[['session_id','item_id', 'serp_pos', 'dist_from_serp', 'item_number', 'serp_len']].merge(session_df, \n",
    "                                                                                       left_on=['session_id','item_number'], \n",
    "                                                                                       right_on=['session_id','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df.hit=corr_df.hit.map(lambda x: 1 if x else 0)\n",
    "corr_df.mrr=corr_df.mrr.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df[['dist_from_serp','serp_pos','hit','mrr','serp_len']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(corr_df[['dist_from_serp','serp_pos','mrr','serp_len']].dropna(), \n",
    "     #     corr_df[['dist_from_serp','serp_pos','hit','mrr','serp_len']].dropna(),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## center/span of item embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average center of embb space\n",
    "items_center=np.average(items_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max distnace between any two items: ', np.linalg.norm(np.max(items_embeddings, axis=0) - np.min(items_embeddings, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERP vs Target clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.serp_pos=clicks_df.serp_pos.map(lambda x: np.nan if x==-1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[clicks_df.session_id.isin(test_session_ids)]\\\n",
    "         .dropna().groupby('session_id').median('serp_pos').serp_pos, bins=100, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[clicks_df.session_id.isin(test_session_ids)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_pos').serp_pos, bins=100, alpha=0.5, label='mean')\n",
    "plt.title('Relative position in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_df['targetXsid']=list(zip(session_df.target.values, session_df.session_id.values))\n",
    "clicks_df['targetXsid']=list(zip(clicks_df.item_number.values, clicks_df.session_id.values))\n",
    "\n",
    "hit_targetXsid=session_df.loc[session_df.hit.values=='True'].targetXsid.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().groupby('session_id').mean('serp_pos').serp_pos, bins=100, label='hit')\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().groupby('session_id').mean('serp_pos').serp_pos, bins=100, label='miss', alpha=0.6)\n",
    "plt.title('Relative position in SERP ranking - lower is better, agg per session')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().serp_pos, bins=100, label='hit')\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().serp_pos, bins=100, label='miss', alpha=0.6)\n",
    "plt.title('Relative position in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[clicks_df.session_id.isin(test_session_ids)]\\\n",
    "         .dropna().groupby('session_id').median('serp_abs_pos').serp_abs_pos, bins=20, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[clicks_df.session_id.isin(test_session_ids)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_abs_pos').serp_abs_pos, bins=20, alpha=0.7, label='mean')\n",
    "plt.title('Absolute position per Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().groupby('session_id').mean('serp_abs_pos').serp_abs_pos, bins=20, label='hit')\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .dropna().groupby('session_id').mean('serp_abs_pos').serp_abs_pos, bins=20, label='miss', alpha=0.6)\n",
    "plt.title('Absolute position in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seperate search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query)]\\\n",
    "         .dropna().groupby('session_id').median('serp_pos').serp_pos, bins=100, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_pos').serp_pos, bins=100, alpha=0.7, label='mean')\n",
    "plt.title('Relative position in search tool Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().groupby('session_id').median('serp_pos').serp_pos, bins=100, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_pos').serp_pos, bins=100, alpha=0.7, label='mean')\n",
    "plt.title('Relative position in non-query Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.search_query.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                  &(clicks_df.search_query)]\\\n",
    "         .dropna().serp_pos, bins=100, label='hit')\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                  &(clicks_df.search_query)]\\\n",
    "         .dropna().serp_pos, bins=100, label='miss', alpha=0.6)\n",
    "plt.title('Relative position in search tool Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().serp_pos, bins=100, label='hit')\n",
    "plt.hist(clicks_df[['session_id','serp_pos']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().serp_pos, bins=100, alpha=0.7, label='miss')\n",
    "plt.title('Relative position in non-query Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query)]\\\n",
    "         .dropna().groupby('session_id').median('serp_abs_pos').serp_abs_pos, bins=100, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_abs_pos').serp_abs_pos, bins=100, alpha=0.7, label='mean')\n",
    "plt.title('Absolute position in search tool Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().groupby('session_id').median('serp_abs_pos').serp_abs_pos, bins=100, label='median')\n",
    "plt.hist(clicks_df[['session_id','serp_abs_pos']].loc[(clicks_df.session_id.isin(test_session_ids))\n",
    "                                                      &(clicks_df.search_query!=True)]\\\n",
    "         .dropna().groupby('session_id').mean('serp_abs_pos').serp_abs_pos, bins=100, alpha=0.7, label='mean')\n",
    "plt.title('Absolute position in non-query Session in SERP ranking - lower is better')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distnace from SERP centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "colors=px.colors.qualitative.Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df[['session_id','dist_from_serp']].loc[(clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .groupby('session_id').median('dist_from_serp').dist_from_serp, bins=100, label='hit')\n",
    "plt.hist(clicks_df[['session_id','dist_from_serp']].loc[(~clicks_df.targetXsid.isin(hit_targetXsid))\n",
    "                                                  &(clicks_df.session_id.isin(test_session_ids))]\\\n",
    "         .groupby('session_id').median('dist_from_serp').dist_from_serp, bins=100, alpha=0.5, label='miss')\n",
    "plt.title('Distance from SERP centre')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ciekawe przypadki\n",
    "# 2137, 10, 1488, 69\n",
    "#1044\n",
    "\n",
    "idx=np.random.choice(list(test_session_ids))\n",
    "\n",
    "sample=clicks_df.loc[clicks_df.session_id==idx]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df.drop_duplicates(subset='serp').target_cluster_proportion)#, bins=np.arange(1, 11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(49+114)/sum([  5,  49,   1,  15,   4, 114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "embs=[]\n",
    "labels=[]\n",
    "for iid in sample.serp.unique()[0].split(','):\n",
    "    try:\n",
    "        embs.append(tsne_items_embeddings[item_dict[int(iid)]])\n",
    "        labels.append(item_labels[item_dict[int(iid)]])\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "embs=np.array(embs)\n",
    "labels=np.array(labels)\n",
    "for label in set(labels):\n",
    "    idxs=labels==label\n",
    "    label_embedding=embs[idxs]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), marker={'color':colors[label]}, mode='markers'))\n",
    "for j,(i,r) in enumerate(sample.iterrows()):\n",
    "    fig.add_trace(go.Scatter(x=[tsne_items_embeddings[r.item_number, 0]], \n",
    "                                y=[tsne_items_embeddings[r.item_number, 1]], \n",
    "                                name=f'item_{j+1}_cluster_{item_labels[r.item_number]}', mode='markers', \n",
    "                                #legendgroup='session_items',\n",
    "                                marker=dict(size=12,\n",
    "                                            color=colors[item_labels[r.item_number]],\n",
    "                                        line=dict(width=2,\n",
    "                                        color='DarkSlateGrey'))))\n",
    "\n",
    "\n",
    "fig.update_layout(title=f'Single SERP items embeddings with GM clusters. Search Query = {is_search_query(sample.query_id.unique()[0])}',\n",
    "                  margin=dict(l=40, r=40, t=40, b=40),\n",
    "                  width=1000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate above info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_serp_clusters_dict={}\n",
    "for _,r in tqdm(clicks_df[['query_id','serp']].drop_duplicates().iterrows()):\n",
    "    labels=[]\n",
    "    if not r.serp is np.nan:\n",
    "        for iid in r.serp.split(','):\n",
    "            try:\n",
    "                labels.append(item_labels[item_dict[int(iid)]])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        query_serp_clusters_dict[r.query_id]=np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serpXcluster_info(r):\n",
    "    if r.item_number<0:\n",
    "        return np.nan, np.nan\n",
    "    try:\n",
    "        vals,counts=query_serp_clusters_dict[r.query_id]\n",
    "        no_vals=vals.shape[0]\n",
    "        proportion=0\n",
    "        if item_labels[r.item_number] in vals:\n",
    "            i=vals.tolist().index(item_labels[r.item_number])\n",
    "            proportion=counts[i]/sum(counts)\n",
    "    except KeyError:\n",
    "        return (np.nan, np.nan)\n",
    "    return no_vals, proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "pom=clicks_df.apply(lambda r: get_serpXcluster_info(r), axis=1)\n",
    "clicks_df['no_clusters_serp']=[x[0] for x in pom]\n",
    "clicks_df['target_cluster_proportion']=[x[1] for x in pom]\n",
    "del pom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicks_df.to_csv('../datasets/diginetica/clicks_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.loc[(clicks_df.target_cluster_proportion>=0.7)\n",
    "              &(clicks_df.session_id.isin(test_session_ids))].shape[0]/clicks_df.loc[clicks_df.session_id.isin(test_session_ids)].target_cluster_proportion.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_df.query_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df.loc[clicks_df.session_id.isin(test_session_ids)].target_cluster_proportion.dropna(), bins=100)\n",
    "plt.title('Proportion of items in SERP belonging to the same GM cluster as target (clicked) item')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(clicks_df.loc[clicks_df.session_id.isin(test_session_ids)].no_clusters_serp.dropna(), bins=100)\n",
    "plt.title('Number of GM clusters in SERP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder on item embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(opt.hiddenSize, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 2), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 8), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 32), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 100), \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, lr\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, opt.hiddenSize)\n",
    "        self.loss=nn.MSELoss()\n",
    "        self.lr=lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = self.loss(x, x_hat)\n",
    "      #  loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=5, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemDataset(data_utils.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        super().__init__()\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxs=np.random.randint(0, item_labels.shape[0], size=int(item_labels.shape[0]*0.8))\n",
    "val_idxs=[x for x in range(item_labels.shape[0]) if not x in train_idxs]\n",
    "\n",
    "items_train_dataloader=data_utils.DataLoader(ItemDataset(items_embeddings[train_idxs], item_labels[train_idxs]),\n",
    "                                             batch_size=32, num_workers=os.cpu_count(),\n",
    "                                             shuffle=True, drop_last=True)\n",
    "items_val_dataloader=data_utils.DataLoader(ItemDataset(items_embeddings[val_idxs], item_labels[val_idxs]),\n",
    "                                             batch_size=32, num_workers=os.cpu_count(),\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=pl.Trainer( accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(monitor=\"val_loss\", mode=\"min\"),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "            EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=opt.patience, mode=\"min\", check_finite=True\n",
    "            ),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder=Autoencoder(lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize parameters\n",
    "    for name, p in autoencoder.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            #p.normal_(0, 0.5)\n",
    "            nn.init.xavier_normal_(p)\n",
    "        elif \"bias\" in name:\n",
    "            p.normal_(0, 1e-2)\n",
    "           # nn.init.xavier_normal_(p)\n",
    "        else:\n",
    "            raise ValueError('Unknown parameter name \"%s\"' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(autoencoder, items_train_dataloader, items_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_item_embeddings=autoencoder.encoder(torch.tensor(items_embeddings, device=autoencoder.device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for label in np.unique(item_labels):\n",
    "    label_embedding=ae_item_embeddings[item_labels==label]\n",
    "    fig.add_trace(go.Scatter(x=label_embedding[:,0], y=label_embedding[:,1], name=str(label), mode='markers'))\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(title='AutoEncoder reduced items embeddings with GM',\n",
    "                  margin=dict(l=40, r=40, t=40, b=40),\n",
    "                  width=1000, height=800)\n",
    "fig.write_html(f'./images/items_AE_{gm.n_components}_{gm.init_params}_{opt.dataset}_{opt.hiddenSize}_{global_run_id.split(\"-\")[-1]}.html')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "del items_train_dataloader\n",
    "del items_val_dataloader\n",
    "del autoencoder\n",
    "del trainer\n",
    "del ae_item_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
